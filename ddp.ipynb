{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DDP: Distributed Data Parallel\n",
    "\n",
    "複数のGPUを活用した分散学習。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分散学習\n",
    "\n",
    "深層学習において、必要な計算を複数のコンピュータに分散させること。いくつかの種類があって、例えばデータを分散させるものとモデルを分散させるもの、またパラメータの更新を同期するか否か、など。今回は、DDPと呼ばれる、モデルを分散させ、パラメータの更新を同期する分散学習について説明する。\n",
    "\n",
    "\\*DDPというのはPyTorchが用意したAPIの名前で、一般的な名前かと言われるとそうではないと思う。ただここではDDPと呼ぶことにする。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$N$個のデータをバッチサイズ$B$で分割し、$M=N/B$個のバッチを得たとする。\n",
    "\n",
    "$R$個のデバイスがあるとき、DDPでは$M$個のバッチを均等に（$M/R$個ずつ）分配する。また各デバイスが同じモデルのコピーを持っているとする。学習が始まると、各デバイスで一つずつバッチを処理する。ここで、バッチを一つ処理する度に各デバイスで勾配を共有し、パラメータを更新する。パラメータが更新されたら次のバッチへ進む。これを繰り返すことで並列的な学習を行う。\n",
    "\n",
    "勾配を共有というのは単に足し合わせているか平均をとっていると思ってよい。単純にバッチサイズが$B\\times R$になったような感じ。各デバイスに同じ勾配が渡るので、更新後のパラメータもデバイス間で同じになる。\n",
    "\n",
    "デバイス間でバッチの処理速度に違いがある場合、遅い方に合わせられる。全てのデバイスがバッチを処理するまで待つということ。\n",
    "\n",
    "このあたりの図解が下記資料に\n",
    "\n",
    "- https://www.cc.u-tokyo.ac.jp/events/lectures/111/20190124-1.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorchでの実装\n",
    "\n",
    "各デバイスで実行するプロセスを呼び出し可能なオブジェクトとして定義し、`torch.multiprocessing`で動かす。デバイス間での勾配の共有には`torch.nn.parallel.DistributedDataParallel`を使う。\n",
    "\n",
    "[Getting Started with Distributed Data Parallel](https://pytorch.org/tutorials/intermediate/ddp_tutorial.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `DistributedSampler`\n",
    "\n",
    "[torch.utils.data.distributed.DistributedSampler](https://pytorch.org/docs/stable/data.html#torch.utils.data.distributed.DistributedSampler)\n",
    "\n",
    "データセットを分割する際に用いる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DistributedSampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "適当なデータセットを用意。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "ds = MyDataset(torch.arange(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここで二つの`Sampler`を用意してみる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler1 = DistributedSampler(ds, num_replicas=2, rank=0, shuffle=False)\n",
    "sampler2 = DistributedSampler(ds, num_replicas=2, rank=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これを使って二つの`DataLoader`を作成する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 2, 4, 6, 8])\n",
      "tensor([10, 12, 14, 16, 18])\n",
      "\n",
      "tensor([1, 3, 5, 7, 9])\n",
      "tensor([11, 13, 15, 17, 19])\n"
     ]
    }
   ],
   "source": [
    "dataloader1 = DataLoader(ds, batch_size=5, sampler=sampler1)\n",
    "dataloader2 = DataLoader(ds, batch_size=5, sampler=sampler2)\n",
    "\n",
    "for x in dataloader1:\n",
    "    print(x)\n",
    "print()\n",
    "for x in dataloader2:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "重複無しで綺麗に二つに分割されている。これを使ってデバイスごとにデータを分割する。\n",
    "\n",
    "`shuffle=True`にするとランダムにデータを分割する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 4, 13,  7,  3,  9])\n",
      "tensor([11, 16, 10, 15,  1])\n",
      "\n",
      "tensor([ 5, 19, 14,  6, 17])\n",
      "tensor([ 2, 18, 12,  8,  0])\n"
     ]
    }
   ],
   "source": [
    "sampler1 = DistributedSampler(ds, num_replicas=2, rank=0, shuffle=True)\n",
    "sampler2 = DistributedSampler(ds, num_replicas=2, rank=1, shuffle=True)\n",
    "dataloader1 = DataLoader(ds, batch_size=5, sampler=sampler1)\n",
    "dataloader2 = DataLoader(ds, batch_size=5, sampler=sampler2)\n",
    "for x in dataloader1:\n",
    "    print(x)\n",
    "print()\n",
    "for x in dataloader2:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ちなみに、何回実行しても同じ分け方になる。中でシードが固定されているのだと思う。分け方を変えたい場合はepochを変える。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 5,  2, 19,  1,  4])\n",
      "tensor([ 0, 16, 15,  6, 12])\n",
      "\n",
      "tensor([13, 11, 18,  9,  7])\n",
      "tensor([14, 10,  3, 17,  8])\n"
     ]
    }
   ],
   "source": [
    "sampler1.set_epoch(1)\n",
    "sampler2.set_epoch(1)\n",
    "for x in dataloader1:\n",
    "    print(x)\n",
    "print()\n",
    "for x in dataloader2:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "データ数がデバイス数で割り切れない場合、余りの数だけ重複を発生させて数を揃える。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 2, 4, 6, 8])\n",
      "tensor([10, 12, 14, 16, 18])\n",
      "tensor([20])\n",
      "\n",
      "tensor([1, 3, 5, 7, 9])\n",
      "tensor([11, 13, 15, 17, 19])\n",
      "tensor([0])\n"
     ]
    }
   ],
   "source": [
    "ds = MyDataset(torch.arange(21))\n",
    "sampler1 = DistributedSampler(ds, num_replicas=2, rank=0, shuffle=False)\n",
    "sampler2 = DistributedSampler(ds, num_replicas=2, rank=1, shuffle=False)\n",
    "dataloader1 = DataLoader(ds, batch_size=5, sampler=sampler1)\n",
    "dataloader2 = DataLoader(ds, batch_size=5, sampler=sampler2)\n",
    "for x in dataloader1:\n",
    "    print(x)\n",
    "print()\n",
    "for x in dataloader2:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `DistributedDataParallel`\n",
    "\n",
    "[Distributed Data Parallel](https://pytorch.org/docs/stable/notes/ddp.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
