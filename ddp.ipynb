{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DDP: Distributed Data Parallel\n",
    "\n",
    "複数のGPUを活用した分散学習。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分散学習\n",
    "\n",
    "深層学習において、必要な計算を複数のコンピュータに分散させること。いくつかの種類があって、例えばデータを分散させるものとモデルを分散させるもの、またパラメータの更新を同期するか否か、など。今回は、DDPと呼ばれる、モデルを分散させ、パラメータの更新を同期する分散学習について説明する。\n",
    "\n",
    "\\*DDPというのはPyTorchが用意したAPIの名前で、一般的な名前かと言われるとそうではないと思う。ただここではDDPと呼ぶことにする。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$N$個のデータをバッチサイズ$B$で分割し、$M=N/B$個のバッチを得たとする。\n",
    "\n",
    "$R$個のデバイスがあるとき、DDPでは$M$個のバッチを均等に（$M/R$個ずつ）分配する。また各デバイスが同じモデルのコピーを持っているとする。学習が始まると、各デバイスで一つずつバッチを処理する。ここで、バッチを一つ処理する度に各デバイスで勾配を共有し、パラメータを更新する。パラメータが更新されたら次のバッチへ進む。これを繰り返すことで並列的な学習を行う。\n",
    "\n",
    "勾配を共有というのは単に足し合わせているか平均をとっていると思ってよい。単純にバッチサイズが$B\\times R$になったような感じ。各デバイスに同じ勾配が渡るので、更新後のパラメータもデバイス間で同じになる。\n",
    "\n",
    "デバイス間でバッチの処理速度に違いがある場合、遅い方に合わせられる。全てのデバイスがバッチを処理するまで待つということ。\n",
    "\n",
    "このあたりの図解が下記資料に\n",
    "\n",
    "- https://www.cc.u-tokyo.ac.jp/events/lectures/111/20190124-1.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorchでの実装\n",
    "\n",
    "各デバイスで実行するプロセスを呼び出し可能なオブジェクトとして定義し、`torch.multiprocessing`で動かす。デバイス間での勾配の共有には`torch.nn.parallel.DistributedDataParallel`を使う。\n",
    "\n",
    "[Getting Started with Distributed Data Parallel](https://pytorch.org/tutorials/intermediate/ddp_tutorial.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `DistributedSampler`\n",
    "\n",
    "[torch.utils.data.distributed.DistributedSampler](https://pytorch.org/docs/stable/data.html#torch.utils.data.distributed.DistributedSampler)\n",
    "\n",
    "データセットを分割する際に用いる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DistributedSampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "適当なデータセットを用意。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "ds = MyDataset(torch.arange(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここで二つの`Sampler`を用意してみる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler1 = DistributedSampler(ds, num_replicas=2, rank=0, shuffle=False)\n",
    "sampler2 = DistributedSampler(ds, num_replicas=2, rank=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`num_replicas`はデバイス数、`rank`はデバイスID。\n",
    "\n",
    "これを使って二つの`DataLoader`を作成する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 2, 4, 6, 8])\n",
      "tensor([10, 12, 14, 16, 18])\n",
      "\n",
      "tensor([1, 3, 5, 7, 9])\n",
      "tensor([11, 13, 15, 17, 19])\n"
     ]
    }
   ],
   "source": [
    "dataloader1 = DataLoader(ds, batch_size=5, sampler=sampler1)\n",
    "dataloader2 = DataLoader(ds, batch_size=5, sampler=sampler2)\n",
    "\n",
    "for x in dataloader1:\n",
    "    print(x)\n",
    "print()\n",
    "for x in dataloader2:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "重複無しで綺麗に二つに分割されている。これを使ってデバイスごとにデータを分割する。\n",
    "\n",
    "`shuffle=True`にするとランダムにデータを分割する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 4, 13,  7,  3,  9])\n",
      "tensor([11, 16, 10, 15,  1])\n",
      "\n",
      "tensor([ 5, 19, 14,  6, 17])\n",
      "tensor([ 2, 18, 12,  8,  0])\n"
     ]
    }
   ],
   "source": [
    "sampler1 = DistributedSampler(ds, num_replicas=2, rank=0, shuffle=True)\n",
    "sampler2 = DistributedSampler(ds, num_replicas=2, rank=1, shuffle=True)\n",
    "dataloader1 = DataLoader(ds, batch_size=5, sampler=sampler1)\n",
    "dataloader2 = DataLoader(ds, batch_size=5, sampler=sampler2)\n",
    "for x in dataloader1:\n",
    "    print(x)\n",
    "print()\n",
    "for x in dataloader2:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "重複はしないようになっている。\n",
    "\n",
    "ちなみに、何回実行しても同じ分け方になる。中でシードが固定されているのだと思う。epochを変えると分け方が変わる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 5,  2, 19,  1,  4])\n",
      "tensor([ 0, 16, 15,  6, 12])\n",
      "\n",
      "tensor([13, 11, 18,  9,  7])\n",
      "tensor([14, 10,  3, 17,  8])\n"
     ]
    }
   ],
   "source": [
    "sampler1.set_epoch(1)\n",
    "sampler2.set_epoch(1)\n",
    "for x in dataloader1:\n",
    "    print(x)\n",
    "print()\n",
    "for x in dataloader2:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データ数がデバイス数で割り切れない場合、余りの数だけ重複を発生させて数を揃えてくれる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 2, 4, 6, 8])\n",
      "tensor([10, 12, 14, 16, 18])\n",
      "tensor([20])\n",
      "\n",
      "tensor([1, 3, 5, 7, 9])\n",
      "tensor([11, 13, 15, 17, 19])\n",
      "tensor([0])\n"
     ]
    }
   ],
   "source": [
    "ds = MyDataset(torch.arange(21)) # 2で割り切れない数にした\n",
    "sampler1 = DistributedSampler(ds, num_replicas=2, rank=0, shuffle=False)\n",
    "sampler2 = DistributedSampler(ds, num_replicas=2, rank=1, shuffle=False)\n",
    "dataloader1 = DataLoader(ds, batch_size=5, sampler=sampler1)\n",
    "dataloader2 = DataLoader(ds, batch_size=5, sampler=sampler2)\n",
    "for x in dataloader1:\n",
    "    print(x)\n",
    "print()\n",
    "for x in dataloader2:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `DistributedDataParallel`\n",
    "\n",
    "勾配を共有してくれる。\n",
    "\n",
    "[Distributed Data Parallel](https://pytorch.org/docs/stable/notes/ddp.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.parallel import DistributedDataParallel as DDP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使い方は簡単で、バックエンドを指定してモデルをラップするだけ。あとは逆伝播を行うときに勝手に勾配を共有してくれる（多分）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import torch.distributed as dist\n",
    "dist.init_process_group(\"nccl\", rank=rank, world_size=n_gpu)\n",
    "model = model.to(rank)\n",
    "model = DDP(model, device_ids=[rank])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>バックエンドを指定して</u>とか言ったけど、俺は全く理解していない。とりあえずGPU使うなら`nccl`指定しておけば良いっぽい。詳細は: [Distributed communication package - torch.distributed](https://pytorch.org/docs/stable/distributed.html)\n",
    "\n",
    "あと追加で環境変数の設定も必要。pythonから設定しちゃえばいいと思う。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import os\n",
    "os.environ[\"MASTER_ADDR\"] = \"localhost\"\n",
    "os.environ[\"MASTER_PORT\"] = \"12355\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "バックエンドや環境変数の設定はおまじないだと思ってコピペしておけばいい。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一つ注意があって、DDPでラップしたモデルは`model.module`でアクセスする必要がある。`model`だけだとDDPのオブジェクトが返ってくる。\n",
    "\n",
    "```python\n",
    "torch.save(model.module.state_dict(), \"model.pth\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `torch.multiprocessing`\n",
    "\n",
    "最後にこれらをまとめる。\n",
    "\n",
    "[torch.multiprocessing](https://pytorch.org/docs/stable/multiprocessing.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下の形で実行する。\n",
    "\n",
    "```python\n",
    "import torch.multiprocessing as mp\n",
    "mp.spawn(\n",
    "    train,\n",
    "    args=(n_gpu, ...),\n",
    "    nprocs=n_gpu,\n",
    "    join=True\n",
    ")\n",
    "```\n",
    "\n",
    "これはjupyterからは動かせないので注意。pythonファイルとして実行する必要がある。\n",
    "\n",
    "あと、`if __name__ == \"__main__\":`の中に書かないと動かない。というかもしかしたらこれを書けばjupyterからも動くかもしれない。試してないから知らんけど。\n",
    "\n",
    "`train`はデバイスごとに実行する関数。`args`は`train`に渡す引数。それ以外はおまじない。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`train`はこんな感じで書けばいいかな。\n",
    "\n",
    "```python\n",
    "def train(rank, n_gpu, model, dataset, n_epochs):\n",
    "    dist.init_process_group(\"nccl\", rank=rank, world_size=n_gpu)\n",
    "    model = model.to(rank)\n",
    "    model = DDP(model, device_ids=[rank])\n",
    "\n",
    "    sampler = DistributedSampler(dataset, num_replicas=n_gpu, rank=rank, shuffle=True)\n",
    "    loader = DataLoader(dataset, sampler=sampler, batch_size=8)\n",
    "    for n in range(n_epochs):\n",
    "        sampler.set_epoch(n)\n",
    "        for x, label in loader:\n",
    "            x = x.to(rank)\n",
    "            label = label.to(rank)\n",
    "            y_pred = model(x)\n",
    "            loss = criterion(y_pred, label)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 実践MNIST\n",
    "\n",
    "実際にMNISTをDDPで学習するサンプルコードを載っけておーしまい。（動作確認してない。あとでやる（どうせやらない）。）\n",
    "\n",
    "↑のように、呼び出し可能なオブジェクトを関数として実装してもいいけど、classでまとめた方が扱い易そうだったのでそうした。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import torch.distributed as dist\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "import torch.multiprocessing as mp\n",
    "\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.relu(self.conv1(x)))\n",
    "        x = self.pool(self.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 64 * 7 * 7)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model, dataset, loss_fn, optimizer, n_gpu, batch_size, n_epochs):\n",
    "        self.model = model\n",
    "        self.dataset = dataset\n",
    "        self.loss_fn = loss_fn\n",
    "        self.optimizer = optimizer\n",
    "        self.n_gpu = n_gpu\n",
    "        self.batch_size = batch_size\n",
    "        self.n_epochs = n_epochs\n",
    "\n",
    "    def setup(self, rank):\n",
    "        dist.init_process_group(\"nccl\", rank=rank, world_size=self.n_gpu)\n",
    "        self.model = self.model.to(rank)\n",
    "        self.model = DDP(self.model, device_ids=[rank])\n",
    "        self.model = torch.compile(self.model)\n",
    "        torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "    def __call__(self, rank):\n",
    "        self.setup(rank)\n",
    "        sampler = DistributedSampler(\n",
    "            self.dataset,\n",
    "            num_replicas=self.n_gpu,\n",
    "            rank=rank,\n",
    "            shuffle=True\n",
    "        )\n",
    "        dataloader = DataLoader(\n",
    "            self.dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            sampler=sampler,\n",
    "            shuffle=(sampler is None), # shuffleはsampler側で決める。この書き方は慣習\n",
    "        )\n",
    "        for n in range(self.n_epochs):\n",
    "            sampler.set_epoch(n)\n",
    "            for x, label in dataloader:\n",
    "                x = x.to(rank)\n",
    "                label = label.to(rank)\n",
    "                self.optimizer.zero_grad()\n",
    "                out = self.model(x)\n",
    "                loss = self.loss_fn(out, label)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "            if rank == 0:\n",
    "                torch.save(self.model._orig_mod.state_dict(), \"model.pth\")\n",
    "\n",
    "\n",
    "def main(args):\n",
    "    os.environ[\"MASTER_ADDR\"] = \"localhost\"\n",
    "    os.environ[\"MASTER_PORT\"] = \"12355\"\n",
    "    dataset = datasets.MNIST(\n",
    "        root='./data',\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=transforms.ToTensor(),\n",
    "    )\n",
    "    model = SimpleCNN()\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    batch_size = 8\n",
    "    n_epochs = 10\n",
    "    trainer = Trainer(\n",
    "        model,\n",
    "        dataset,\n",
    "        loss_fn,\n",
    "        optimizer,\n",
    "        args.n_gpus,\n",
    "        batch_size,\n",
    "        n_epochs\n",
    "    )\n",
    "    mp.spawn(\n",
    "        trainer,\n",
    "        nprocs=args.n_gpus,\n",
    "        join=True,\n",
    "    )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--n_gpus\", type=int)\n",
    "    args = parser.parse_args()\n",
    "    main(args)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
